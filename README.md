# Explainable Monotonic Networks and Constrained Learning for Interpretable Classification and Weakly Supervised Anomaly Detection

*Please cite : Wargnier-Dauchelle, Valentine, et al. "Explainable Monotonic Networks and Constrained Learning for Interpretable Classification and Weakly Supervised Anomaly Detection." Submitted to Pattern Recognition.*
 
**Abstract**: Deep networks interpretability is fundamental in critical domains like medicine: using easily explainable networks with decisions based on radiological signs and not on spurious confounders would reassure the clinicians. Confidence is reinforced by the integration of intrinsic properties and characteristics of monotonic networks could be used to design such intrinsically explainable networks. As they are considered as too constrained and difficult to train, they are often very shallow and rarely used for image applications. In this work, we propose a procedure to transform any architecture into a trainable monotonic network, identifying the critical importance of weights initialization, and highlight the interest of such networks for explicability and interpretability. By constraining the features and the gradients of a healthy vs pathological images classifier, we show, using counterfactual examples, that the network decision is more based on the radiological signs of the pathology and outperforms state-of-the-art methods for weakly supervised anomaly detection.

